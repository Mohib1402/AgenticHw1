{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Set up ADK and Gemini API key"
      ],
      "metadata": {
        "id": "U2lWR22Ho5zh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_o-FvrtDWll",
        "outputId": "0155cb0c-75d1-4469-97d2-4b956498fbfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "âš ï¸ Set KEY to your real Gemini API key (AI Studio).\n",
            "adk, version 1.16.0\n"
          ]
        }
      ],
      "source": [
        "!pip -q install --upgrade \"google-adk==1.16.0\"\n",
        "\n",
        "import os, sys, subprocess\n",
        "print(\"Python:\", sys.version)\n",
        "\n",
        "# ğŸ”‘ Put your Gemini API key here (or set GOOGLE_API_KEY in environment)\n",
        "KEY = os.environ.get(\"GOOGLE_API_KEY\", \"\").strip() or \"\"\n",
        "if KEY == \"\":\n",
        "    print(\"âš ï¸ Set KEY to your real Gemini API key (AI Studio).\")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = KEY\n",
        "\n",
        "# Force non-Vertex to avoid any surprises\n",
        "for var in [\"GOOGLE_GENAI_USE_VERTEXAI\",\"GOOGLE_VERTEX_PROJECT\",\"GOOGLE_VERTEX_LOCATION\",\"GOOGLE_CLOUD_PROJECT\"]:\n",
        "    os.environ.pop(var, None)\n",
        "\n",
        "# Sanity\n",
        "print(subprocess.run([\"adk\",\"--version\"], capture_output=True, text=True).stdout.strip())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create the Job Interview Agent package"
      ],
      "metadata": {
        "id": "Sk3s8kNEo7s0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pathlib, shutil, textwrap\n",
        "\n",
        "pkg = \"job_interview_agent\"\n",
        "shutil.rmtree(pkg, ignore_errors=True)\n",
        "pathlib.Path(pkg).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "open(f\"{pkg}/__init__.py\",\"w\").write(\"from .agent import root_agent\\n\")\n",
        "\n",
        "# 2a) Question bank\n",
        "open(f\"{pkg}/question_bank.py\",\"w\").write(textwrap.dedent(\"\"\"\n",
        "from typing import List, Dict\n",
        "\n",
        "QUESTION_BANK: Dict[str, list[str]] = {\n",
        "    \"data-analyst\": [\n",
        "        \"How would you handle missing data and outliers?\",\n",
        "        \"Explain the difference between left, right, and inner joins.\",\n",
        "        \"When would you choose median over mean? Give an example.\"\n",
        "    ],\n",
        "    \"frontend\": [\n",
        "        \"How do you structure a React component for reusability?\",\n",
        "        \"Explain hydration and server-side rendering.\",\n",
        "        \"Whatâ€™s your approach to accessibility testing?\"\n",
        "    ],\n",
        "    \"generic\": [\n",
        "        \"Tell me about the proudest accomplishment in your career.\",\n",
        "        \"How do you handle conflict within a team?\",\n",
        "        \"Describe a time you made a hard trade-off.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "def get_questions(role: str, count: int = 3) -> list[str]:\n",
        "    role = (role or \"\").strip().lower()\n",
        "    bank = QUESTION_BANK.get(role, QUESTION_BANK[\"generic\"])\n",
        "    count = max(1, min(count, len(bank)))\n",
        "    return bank[:count]\n",
        "\"\"\"))\n",
        "\n",
        "# 2b) Tools that return FINAL formatted text\n",
        "open(f\"{pkg}/interview_tools.py\",\"w\").write(textwrap.dedent(\"\"\"\n",
        "from typing import Tuple\n",
        "from .question_bank import QUESTION_BANK, get_questions as _get_qs\n",
        "\n",
        "def get_questions(role: str, count: int = 3) -> str:\n",
        "    qs = _get_qs(role, count)\n",
        "    lines = [\"Questions:\"]\n",
        "    for i, q in enumerate(qs, start=1):\n",
        "        lines.append(f\"{i}. {q}\")\n",
        "    return \"\\\\n\".join(lines)\n",
        "\n",
        "def _score(answer: str) -> Tuple[int, str]:\n",
        "    if not answer or len(answer.strip()) < 20:\n",
        "        return 1, \"Answer is too short; add detail and rationale.\"\n",
        "    score = 2\n",
        "    lower = answer.lower()\n",
        "    if any(k in lower for k in (\"because\", \"why\", \"tradeoff\", \"trade-off\", \"therefore\")):\n",
        "        score += 1\n",
        "    if any(k in lower for k in (\"example\", \"metric\", \"latency\", \"accuracy\", \"roi\", \"throughput\", \"lcp\", \"cls\", \"tbt\")):\n",
        "        score += 1\n",
        "    if len(answer.split()) > 120:\n",
        "        score = min(score, 4)\n",
        "    score = max(0, min(score, 5))\n",
        "    fb = \"Clear reasoning with concrete details.\" if score >= 4 else \"Clarify reasoning and add specifics.\"\n",
        "    return score, fb\n",
        "\n",
        "# Make 'question' optional so the tool still works if the LLM omits it.\n",
        "def score_answer(answer: str, question: str = \"\") -> str:\n",
        "    score, fb = _score(answer or \"\")\n",
        "    return f\"Score: {score}. Feedback: {fb}\"\n",
        "\"\"\"))\n",
        "\n",
        "# 2c) Agent: prints tool output VERBATIM\n",
        "open(f\"{pkg}/agent.py\",\"w\").write(textwrap.dedent(\"\"\"\n",
        "import os\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.tools import FunctionTool\n",
        "from .interview_tools import get_questions, score_answer\n",
        "\n",
        "# Use the environment key already set in Step 1.\n",
        "# Ensure Vertex is off\n",
        "for var in [\"GOOGLE_GENAI_USE_VERTEXAI\",\"GOOGLE_VERTEX_PROJECT\",\"GOOGLE_VERTEX_LOCATION\",\"GOOGLE_CLOUD_PROJECT\"]:\n",
        "    os.environ.pop(var, None)\n",
        "\n",
        "root_agent = Agent(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    name=\"job_interview_agent\",\n",
        "    description=\"Generates interview questions and scores answers.\",\n",
        "    instruction=(\n",
        "        \"You have two tools: get_questions(role, count) and score_answer(answer, question?).\\\\n\"\n",
        "        \"ROUTING:\\\\n\"\n",
        "        \"- If the user asks to score/grade an answer, you MUST call score_answer.\\\\n\"\n",
        "        \"- Otherwise, if they ask for interview questions, call get_questions.\\\\n\"\n",
        "        \"OUTPUT:\\\\n\"\n",
        "        \"- Print the tool's return value VERBATIM with NO extra words before/after.\\\\n\"\n",
        "        \"- NEVER output acknowledgements like 'Okay.' or any pre/post text.\\\\n\"\n",
        "        \"FORMATS (already produced by tools):\\\\n\"\n",
        "        \"- Questions:\\\\n1. ...\\\\n2. ...\\\\n\"\n",
        "        \"- Score: <0-5>. Feedback: <one sentence>\\\\n\"\n",
        "    ),\n",
        "    tools=[FunctionTool(get_questions), FunctionTool(score_answer)],\n",
        ")\n",
        "\"\"\"))\n",
        "\n",
        "print(\"[OK] Scaffolding complete.\")\n",
        "!ls -la {pkg}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7KrY-uXY859",
        "outputId": "d47db6ab-4e98-4e1e-957a-40d3c79be494"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Scaffolding complete.\n",
            "total 24\n",
            "drwxr-xr-x 2 root root 4096 Oct 20 06:52 .\n",
            "drwxr-xr-x 1 root root 4096 Oct 20 06:52 ..\n",
            "-rw-r--r-- 1 root root 1224 Oct 20 06:52 agent.py\n",
            "-rw-r--r-- 1 root root   30 Oct 20 06:52 __init__.py\n",
            "-rw-r--r-- 1 root root 1194 Oct 20 06:52 interview_tools.py\n",
            "-rw-r--r-- 1 root root  933 Oct 20 06:52 question_bank.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Harden agent: tool-only, deterministic"
      ],
      "metadata": {
        "id": "VdZ2UPsyo-O7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "Path(\"job_interview_agent/agent.py\").write_text(\"\"\"\n",
        "import os\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.tools import FunctionTool\n",
        "from .interview_tools import get_questions, score_answer\n",
        "\n",
        "# Ensure we never route to Vertex\n",
        "for var in [\"GOOGLE_GENAI_USE_VERTEXAI\",\"GOOGLE_VERTEX_PROJECT\",\"GOOGLE_VERTEX_LOCATION\",\"GOOGLE_CLOUD_PROJECT\"]:\n",
        "    os.environ.pop(var, None)\n",
        "\n",
        "root_agent = Agent(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    # Make the model deterministic and conservative\n",
        "    model_params={\"temperature\": 0.0, \"top_p\": 0.0},\n",
        "    name=\"job_interview_agent\",\n",
        "    description=\"Generates interview questions and scores answers. Tool-only; prints tool output verbatim.\",\n",
        "    instruction=(\n",
        "        \"TOOL-ONLY MODE. For EVERY user message, you MUST call EXACTLY ONE tool and return its STRING output VERBATIM.\\\\n\"\n",
        "        \"Never type your own prose. Never preface with 'Okay'. Never add extra lines.\\\\n\"\n",
        "        \"\\\\n\"\n",
        "        \"TOOLS AVAILABLE:\\\\n\"\n",
        "        \"- get_questions(role: string, count?: int) -> str  (prints 'Questions:\\\\n1. ...\\\\n2. ...')\\\\n\"\n",
        "        \"- score_answer(answer: string, question?: string) -> str  (prints 'Score: <0-5>. Feedback: <...>')\\\\n\"\n",
        "        \"\\\\n\"\n",
        "        \"ROUTING RULES (in priority order):\\\\n\"\n",
        "        \"1) If the message contains a literal call like get_questions(â€¦) or score_answer(â€¦), extract the args and call THAT tool EXACTLY. DO NOT swap tools.\\\\n\"\n",
        "        \"2) Else if the message contains words like 'score' or 'grade' or provides an 'Answer:', call score_answer(answer=?, question=? if present).\\\\n\"\n",
        "        \"3) Otherwise, default to get_questions and infer role/count if needed.\\\\n\"\n",
        "        \"\\\\n\"\n",
        "        \"STRICT OUTPUT: Return the tool's return string EXACTLY. No extra words before or after.\\\\n\"\n",
        "        \"\\\\n\"\n",
        "        \"FEW-SHOT EXAMPLES (DO NOT ECHO):\\\\n\"\n",
        "        \"USER: Call get_questions(role='frontend', count=2) and print verbatim.\\\\n\"\n",
        "        \"ASSISTANT -> (tool:get_questions role=frontend count=2) -> returns 'Questions:\\\\n1. ...\\\\n2. ...'\\\\n\"\n",
        "        \"ASSISTANT: Questions:\\\\n1. ...\\\\n2. ...\\\\n\"\n",
        "        \"\\\\n\"\n",
        "        \"USER: Call score_answer(answer='SSR...', question='SSR vs CSR?') and print verbatim.\\\\n\"\n",
        "        \"ASSISTANT -> (tool:score_answer answer='SSR...' question='SSR vs CSR?') -> returns 'Score: 4. Feedback: ...'\\\\n\"\n",
        "        \"ASSISTANT: Score: 4. Feedback: ...\\\\n\"\n",
        "    ),\n",
        "    tools=[FunctionTool(get_questions), FunctionTool(score_answer)],\n",
        ")\n",
        "\"\"\")\n",
        "print(\"[OK] agent hardened (tool-only, temp=0)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSQKFTKyZ2kl",
        "outputId": "ef6351ed-4727-4129-c806-43a5a2289547"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] agent hardened (tool-only, temp=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Finalize agent config (remove model params)"
      ],
      "metadata": {
        "id": "lDcTnc4xpBOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "Path(\"job_interview_agent/agent.py\").write_text(\"\"\"\n",
        "import os\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.tools import FunctionTool\n",
        "from .interview_tools import get_questions, score_answer\n",
        "\n",
        "# Keep requests on Gemini API (not Vertex) if any envs are present\n",
        "for var in [\"GOOGLE_GENAI_USE_VERTEXAI\",\"GOOGLE_VERTEX_PROJECT\",\"GOOGLE_VERTEX_LOCATION\",\"GOOGLE_CLOUD_PROJECT\"]:\n",
        "    os.environ.pop(var, None)\n",
        "\n",
        "root_agent = Agent(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    name=\"job_interview_agent\",\n",
        "    description=\"Generates interview questions and scores answers. Tool-only; prints tool output verbatim.\",\n",
        "    instruction=(\n",
        "        \"TOOL-ONLY MODE. For EVERY user message, you MUST call EXACTLY ONE tool and return its STRING output VERBATIM.\\\\n\"\n",
        "        \"Never type your own prose. Never add extra words.\\\\n\"\n",
        "        \"\\\\n\"\n",
        "        \"TOOLS:\\\\n\"\n",
        "        \"- get_questions(role: string, count?: int) -> str  (returns 'Questions:\\\\n1. ...\\\\n2. ...')\\\\n\"\n",
        "        \"- score_answer(answer: string, question?: string) -> str  (returns 'Score: <0-5>. Feedback: <...>')\\\\n\"\n",
        "        \"\\\\n\"\n",
        "        \"ROUTING (priority):\\\\n\"\n",
        "        \"1) If the user gives a literal call like get_questions(...) or score_answer(...), extract those args and call THAT tool.\\\\n\"\n",
        "        \"2) Else if the message says 'score'/'grade' or includes an 'Answer:', call score_answer(answer=?, question=? if present).\\\\n\"\n",
        "        \"3) Otherwise, default to get_questions and infer role/count if needed.\\\\n\"\n",
        "        \"\\\\n\"\n",
        "        \"STRICT OUTPUT: Return the tool's return string EXACTLY.\\\\n\"\n",
        "    ),\n",
        "    tools=[FunctionTool(get_questions), FunctionTool(score_answer)],\n",
        ")\n",
        "\"\"\")\n",
        "print(\"[OK] Removed model_params; agent hardened\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dDyyNfeaZmM",
        "outputId": "c1222c71-033d-4d94-e0f9-f43bca34969e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Removed model_params; agent hardened\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CLI sanity checks (generate & score)"
      ],
      "metadata": {
        "id": "rpVqpt42pDtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess, re, os\n",
        "\n",
        "def run_adk_once(agent_pkg: str, user_message: str) -> str:\n",
        "    payload = user_message.strip() + \"\\nexit\\n\"\n",
        "    p = subprocess.run(\n",
        "        [\"adk\",\"run\",agent_pkg],\n",
        "        input=payload.encode(),\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        check=False,\n",
        "        env=dict(os.environ),\n",
        "    )\n",
        "    out = p.stdout.decode(\"utf-8\", \"replace\")\n",
        "    # Keep just the agentâ€™s reply\n",
        "    m = re.search(rf\"\\[{re.escape(agent_pkg)}\\]:\\s*(.*?)(?:\\n\\[user\\]:|\\Z)\", out, re.S)\n",
        "    return (m.group(1).strip() if m else out).strip()\n",
        "\n",
        "print(\">>> Q generation\")\n",
        "print(run_adk_once(\n",
        "    \"job_interview_agent\",\n",
        "    \"Call get_questions(role='frontend', count=2) and print the tool output verbatim.\"\n",
        "))\n",
        "\n",
        "print(\"\\n>>> Scoring\")\n",
        "print(run_adk_once(\n",
        "    \"job_interview_agent\",\n",
        "    \"Call score_answer(answer='SSR renders on the server; hydration attaches event handlers on the client. \"\n",
        "    \"I compare LCP/TBT/CLS and conversion impact, choosing based on SEO & latency.') and print the tool output verbatim.\"\n",
        "))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaNhFH4OY_Lv",
        "outputId": "a827353b-3f03-427b-8927-6c1e156d7440"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Q generation\n",
            "Questions:\n",
            "1. How do you structure a React component for reusability?\n",
            "2. Explain hydration and server-side rendering.\n",
            "\n",
            ">>> Scoring\n",
            "Score: 3. Feedback: Clarify reasoning and add specifics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Automated smoke tests"
      ],
      "metadata": {
        "id": "XpOM6bSepN80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tests_smoke.py\n",
        "import re, os, subprocess\n",
        "\n",
        "def run_once(agent_pkg: str, msg: str) -> str:\n",
        "    p = subprocess.run(\n",
        "        [\"adk\",\"run\",agent_pkg],\n",
        "        input=(msg.strip()+\"\\nexit\\n\").encode(),\n",
        "        stdout=subprocess.PIPE, stderr=subprocess.STDOUT, check=False, env=os.environ,\n",
        "    )\n",
        "    out = p.stdout.decode(\"utf-8\",\"replace\")\n",
        "    m = re.search(rf\"\\[{re.escape(agent_pkg)}\\]:\\s*(.*?)(?:\\n\\[user\\]:|\\Z)\", out, re.S)\n",
        "    return (m.group(1).strip() if m else out).strip()\n",
        "\n",
        "def expect(txt, patterns):\n",
        "    missing = [p for p in patterns if not re.search(p, txt, re.S)]\n",
        "    if missing:\n",
        "        raise AssertionError(f\"Missing: {missing}\\n-- Got --\\n{txt}\")\n",
        "\n",
        "# Tests\n",
        "qs = run_once(\"job_interview_agent\",\n",
        "              \"Call get_questions(role='frontend', count=2) and print the tool output verbatim.\")\n",
        "expect(qs, [r\"^Questions:\\s*\\n1\\.\", r\"\\n2\\.\"])\n",
        "\n",
        "sc = run_once(\"job_interview_agent\",\n",
        "              \"Call score_answer(answer='SSR renders on the server; hydration attaches client handlers. \"\n",
        "              \"I track LCP/TBT/CLS and pick based on SEO/latency.') and print the tool output verbatim.\")\n",
        "expect(sc, [r\"^Score:\\s*[0-5]\\b\", r\"Feedback:\\s*.+\"])\n",
        "\n",
        "print(\"Smoke tests passed âœ…\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8FcBSG-f51t",
        "outputId": "b4282f53-aeae-425a-9cec-91ad3e66166c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Smoke tests passed âœ…\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Demo runner: save sample outputs"
      ],
      "metadata": {
        "id": "LvMQcNx5pSJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > demo_runner.py <<'PY'\n",
        "import os, re, subprocess, datetime\n",
        "\n",
        "def run_once(agent_pkg, msg):\n",
        "    p = subprocess.run(\n",
        "        [\"adk\",\"run\",agent_pkg],\n",
        "        input=(msg.strip()+\"\\nexit\\n\").encode(),\n",
        "        stdout=subprocess.PIPE, stderr=subprocess.STDOUT, check=False, env=os.environ\n",
        "    )\n",
        "    out = p.stdout.decode(\"utf-8\",\"replace\")\n",
        "    m = re.search(rf\"\\[{re.escape(agent_pkg)}\\]:\\s*(.*?)(?:\\n\\[user\\]:|\\Z)\", out, re.S)\n",
        "    return (m.group(1).strip() if m else out).strip()\n",
        "\n",
        "q_prompt = \"Call get_questions(role='data-analyst', count=3) and print the tool output verbatim.\"\n",
        "s_prompt = (\"Call score_answer(answer='Normalization reduces redundancy; denormalization speeds reads. \"\n",
        "            \"I use 3NF for OLTP, denormalize for OLAP; validate with query profiling.') \"\n",
        "            \"and print the tool output verbatim.\")\n",
        "\n",
        "qs = run_once(\"job_interview_agent\", q_prompt)\n",
        "sc = run_once(\"job_interview_agent\", s_prompt)\n",
        "\n",
        "ts = datetime.datetime.now().isoformat(timespec=\"seconds\")\n",
        "with open(\"demo_output.txt\",\"w\") as f:\n",
        "    f.write(f\"=== Demo run @ {ts} ===\\n\\n\")\n",
        "    f.write(\"Prompt 1 (Questions):\\n\" + q_prompt + \"\\n\\n\")\n",
        "    f.write(\"Output 1:\\n\" + qs + \"\\n\\n\")\n",
        "    f.write(\"Prompt 2 (Scoring):\\n\" + s_prompt + \"\\n\\n\")\n",
        "    f.write(\"Output 2:\\n\" + sc + \"\\n\")\n",
        "\n",
        "print(\"Wrote demo_output.txt\")\n",
        "PY\n",
        "\n",
        "python demo_runner.py\n",
        "sed -n '1,200p' demo_output.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNQus4Rzh9Lq",
        "outputId": "e3b95b84-a59b-40b2-c13b-c71bca7bf377"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote demo_output.txt\n",
            "=== Demo run @ 2025-10-20T07:34:57 ===\n",
            "\n",
            "Prompt 1 (Questions):\n",
            "Call get_questions(role='data-analyst', count=3) and print the tool output verbatim.\n",
            "\n",
            "Output 1:\n",
            "Questions:\n",
            "1. How would you handle missing data and outliers?\n",
            "2. Explain the difference between left, right, and inner joins.\n",
            "3. When would you choose median over mean? Give an example.\n",
            "\n",
            "Prompt 2 (Scoring):\n",
            "Call score_answer(answer='Normalization reduces redundancy; denormalization speeds reads. I use 3NF for OLTP, denormalize for OLAP; validate with query profiling.') and print the tool output verbatim.\n",
            "\n",
            "Output 2:\n",
            "Score: 2. Feedback: Clarify reasoning and add specifics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Standalone test script: write & run"
      ],
      "metadata": {
        "id": "RaXW4M1NpVUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > tests_smoke.py << 'PY'\n",
        "import re, os, subprocess\n",
        "\n",
        "def run_once(agent_pkg: str, msg: str) -> str:\n",
        "    p = subprocess.run(\n",
        "        [\"adk\",\"run\",agent_pkg],\n",
        "        input=(msg.strip()+\"\\nexit\\n\").encode(),\n",
        "        stdout=subprocess.PIPE, stderr=subprocess.STDOUT, check=False, env=os.environ,\n",
        "    )\n",
        "    out = p.stdout.decode(\"utf-8\",\"replace\")\n",
        "    m = re.search(rf\"\\[{re.escape(agent_pkg)}\\]:\\s*(.*?)(?:\\n\\[user\\]:|\\Z)\", out, re.S)\n",
        "    return (m.group(1).strip() if m else out).strip()\n",
        "\n",
        "def expect(txt, patterns):\n",
        "    missing = [p for p in patterns if not re.search(p, txt, re.S)]\n",
        "    if missing:\n",
        "        raise AssertionError(f\"Missing: {missing}\\n-- Got --\\n{txt}\")\n",
        "\n",
        "qs = run_once(\"job_interview_agent\",\n",
        "              \"Call get_questions(role='frontend', count=2) and print the tool output verbatim.\")\n",
        "expect(qs, [r\"^Questions:\\s*\\n1\\.\", r\"\\n2\\.\"])\n",
        "\n",
        "sc = run_once(\"job_interview_agent\",\n",
        "              \"Call score_answer(answer='SSR renders on the server; hydration attaches client handlers. \"\n",
        "              \"I track LCP/TBT/CLS and pick based on SEO/latency.') and print the tool output verbatim.\")\n",
        "expect(sc, [r\"^Score:\\s*[0-5]\\b\", r\"Feedback:\\s*.+\"])\n",
        "\n",
        "print(\"Smoke tests passed âœ…\")\n",
        "PY\n",
        "\n",
        "python tests_smoke.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8AidW-iiM1A",
        "outputId": "23694ce7-f73b-44d4-a1a4-60548c26ebed"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Smoke tests passed âœ…\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pin websockets and install Gradio"
      ],
      "metadata": {
        "id": "kuHFbBJDpXD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Pin websockets to satisfy both ADK and google-genai\n",
        "!pip -q install --upgrade \"websockets==15.0.1\"\n",
        "\n",
        "# 2) (Re)install gradio but keep our pinned websockets\n",
        "!pip -q install --upgrade \"gradio==5.4.0\"\n",
        "\n",
        "# 3) Make sure Python actually uses the new websockets module in this session\n",
        "import sys\n",
        "for m in [m for m in list(sys.modules) if m.startswith(\"websockets\")]:\n",
        "    del sys.modules[m]\n",
        "\n",
        "import websockets\n",
        "print(\"websockets version now:\", websockets.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwWrjlffkIAa",
        "outputId": "3e2a4e67-dba2-475b-d2e2-53c573bb1867"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/182.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”\u001b[0m \u001b[32m174.1/182.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m182.5/182.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gradio-client 1.4.2 requires websockets<13.0,>=10.0, but you have websockets 15.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.16.0 requires websockets<16.0.0,>=15.0.1, but you have websockets 12.0 which is incompatible.\n",
            "yfinance 0.2.66 requires websockets>=13.0, but you have websockets 12.0 which is incompatible.\n",
            "dataproc-spark-connect 0.8.3 requires websockets>=14.0, but you have websockets 12.0 which is incompatible.\n",
            "google-genai 1.44.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mwebsockets version now: 12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradio UI for the Interview Agent"
      ],
      "metadata": {
        "id": "x2rTAxwxpdGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Robust Gradio UI that shells out to ADK CLI (no async) ---\n",
        "\n",
        "import gradio as gr\n",
        "import subprocess, re, os\n",
        "\n",
        "AGENT = \"job_interview_agent\"\n",
        "\n",
        "def _run_adk_once(agent_pkg: str, user_message: str) -> str:\n",
        "    try:\n",
        "        payload = user_message.strip() + \"\\nexit\\n\"\n",
        "        p = subprocess.run(\n",
        "            [\"adk\", \"run\", agent_pkg],\n",
        "            input=payload.encode(),\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            check=False,\n",
        "            env=dict(os.environ),\n",
        "        )\n",
        "        out = p.stdout.decode(\"utf-8\", \"replace\")\n",
        "        m = re.search(rf\"\\[{re.escape(agent_pkg)}\\]:\\s*(.*?)(?:\\n\\[user\\]:|\\Z)\", out, re.S)\n",
        "        return (m.group(1).strip() if m else out).strip()\n",
        "    except Exception as e:\n",
        "        return f\"(error) {type(e).__name__}: {e}\"\n",
        "\n",
        "def ask_questions(role: str, count: int) -> str:\n",
        "    msg = f\"Call get_questions(role='{role}', count={int(count)}) and print the tool output verbatim.\"\n",
        "    return _run_adk_once(AGENT, msg)\n",
        "\n",
        "def score_answer(question: str, answer: str) -> str:\n",
        "    answer = (answer or \"\").strip()\n",
        "    if not answer:\n",
        "        return \"Score: 1. Feedback: Answer is too short; add detail and rationale.\"\n",
        "    if question and question.strip():\n",
        "        msg = f\"Call score_answer(answer='{answer}', question='{question}') and print the tool output verbatim.\"\n",
        "    else:\n",
        "        msg = f\"Call score_answer(answer='{answer}') and print the tool output verbatim.\"\n",
        "    return _run_adk_once(AGENT, msg)\n",
        "\n",
        "with gr.Blocks(title=\"Job Interview Agent (ADK)\") as demo:\n",
        "    gr.Markdown(\"## Job Interview Agent Â· ADK + Gemini\\nGenerate interview questions and score answers using your agent tools.\")\n",
        "\n",
        "    with gr.Tab(\"Generate Questions\"):\n",
        "        with gr.Row():\n",
        "            role = gr.Dropdown(choices=[\"frontend\",\"data-analyst\",\"generic\"], value=\"frontend\", label=\"Role\")\n",
        "            count = gr.Slider(1, 5, value=2, step=1, label=\"Number of questions\")\n",
        "        out_q = gr.Textbox(label=\"Questions\", lines=8)\n",
        "        gr.Button(\"Generate\").click(ask_questions, inputs=[role, count], outputs=out_q)\n",
        "\n",
        "    with gr.Tab(\"Score Answer\"):\n",
        "        q = gr.Textbox(label=\"Question (optional)\", placeholder=\"e.g., Explain hydration and SSR.\")\n",
        "        a = gr.Textbox(label=\"Your Answer\", lines=6, placeholder=\"Paste or type your answer hereâ€¦\")\n",
        "        out_s = gr.Textbox(label=\"Score & Feedback\", lines=2)\n",
        "        gr.Button(\"Score\").click(score_answer, inputs=[q, a], outputs=out_s)\n",
        "\n",
        "# In Colab it's safer to enable share + debug to see errors inline if any\n",
        "demo.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "0ovWDHAqk0Ax",
        "outputId": "81686541-ef59-4404-817c-3347cf234353"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://069bab35f4a719d7c2.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://069bab35f4a719d7c2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    }
  ]
}